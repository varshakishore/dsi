{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367695cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f5ecd6",
   "metadata": {},
   "source": [
    "# Fileter and split generated queries into seen and unseen set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82036f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter generated queries \n",
    "file1 = open('/home/vk352/ANCE/NQ320k_dataset_v2/passages_30.json', 'r')\n",
    "lines = file1.readlines()\n",
    "dictq = defaultdict(set)\n",
    "for line in tqdm(lines):\n",
    "    item = json.loads(line)\n",
    "    dictq[item['gen_question']].add(item['doc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b53fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = []\n",
    "for k, v in dictq.items():\n",
    "    if len(v)>1:\n",
    "        duplicates.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = set(duplicates)\n",
    "passages_id = passages_unseen_id = 0\n",
    "file =  open('/home/vk352/ANCE/NQ320k_dataset_v2/passages_seen.json', 'w')\n",
    "file1 = open('/home/vk352/ANCE/NQ320k_dataset_v2/passages_unseen.json', 'w')\n",
    "for i in tqdm(range(0, len(lines), 30)):\n",
    "    lines_block = lines[i:i+30]\n",
    "    random.shuffle(lines_block)\n",
    "    j = 0\n",
    "    for k in range(30):\n",
    "        item = json.loads(lines_block[k])\n",
    "        if item['gen_question'] not in duplicates:\n",
    "            item['gen_ques_id'] = passages_id\n",
    "            json.dump(item, file)\n",
    "            file.write('\\n')\n",
    "            passages_id += 1\n",
    "            j += 1\n",
    "        if j== 15: break\n",
    "    for l in range(k+1, 30):\n",
    "        item = json.loads(lines_block[l])\n",
    "        if item['gen_question'] not in duplicates:\n",
    "            item['gen_ques_id'] = passages_unseen_id\n",
    "            json.dump(item, file1)\n",
    "            file1.write('\\n')\n",
    "            passages_unseen_id += 1\n",
    "file.close()\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38eb182",
   "metadata": {},
   "source": [
    "# Generate old, new and tune document splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf421b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c7dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(109715) # there are 109715 docs in NQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e02017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 109715\n",
    "percent_90 = int(0.9* num_docs)\n",
    "percent_99 = int(0.99* num_docs)\n",
    "old_docs = perm[:percent_90]\n",
    "new_docs = perm[percent_90:percent_99]\n",
    "tune_docs = perm[percent_99:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7fb247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98743, 9874, 1098)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_docs), len(new_docs), len(tune_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a352caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7830/7830 [00:00<00:00, 17794.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of file: testqueries\n",
      "Number in Old Docs: 6998\n",
      "Number in New Docs: 738\n",
      "Number in Tune Docs: 94\n",
      "Total: 7830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307373/307373 [00:15<00:00, 19861.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of file: naturalqueries\n",
      "Number in Old Docs: 276493\n",
      "Number in New Docs: 27723\n",
      "Number in Tune Docs: 3157\n",
      "Total: 307373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1645085/1645085 [01:23<00:00, 19698.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of file: passages_seen\n",
      "Number in Old Docs: 1480538\n",
      "Number in New Docs: 148077\n",
      "Number in Tune Docs: 16470\n",
      "Total: 1645085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1471338/1471338 [01:14<00:00, 19754.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of file: passages_unseen\n",
      "Number in Old Docs: 1324114\n",
      "Number in New Docs: 132415\n",
      "Number in Tune Docs: 14809\n",
      "Total: 1471338\n"
     ]
    }
   ],
   "source": [
    "typelist = ['testqueries', 'trainqueries', 'passages_seen', 'passages_unseen']\n",
    "# typelist = ['testqueries']\n",
    "\n",
    "for type_file in typelist:\n",
    "    new_folder = '/home/vk352/dsi/data/NQ320k/'\n",
    "    old_id = new_id = tune_id = 0\n",
    "    file = open(f'/home/vk352/ANCE/NQ320k_dataset_v2/{type_file}.json', 'r')\n",
    "    if type_file=='trainqueries':\n",
    "        type_file = 'naturalqueries'\n",
    "    old_docs_file = open(os.path.join(new_folder, 'old_docs', f'{type_file}.json'), 'w')\n",
    "    new_docs_file = open(os.path.join(new_folder, 'new_docs', f'{type_file}.json'), 'w')\n",
    "    tune_docs_file = open(os.path.join(new_folder, 'tune_docs', f'{type_file}.json'), 'w')\n",
    "    \n",
    "    lines = file.readlines()\n",
    "\n",
    "    for line in tqdm(lines):\n",
    "        item = json.loads(line)\n",
    "        if item[\"doc_id\"] in old_docs:\n",
    "            item['ques_id'] = old_id\n",
    "            json.dump(item, old_docs_file)\n",
    "            old_docs_file.write('\\n')\n",
    "            old_id += 1\n",
    "        elif item[\"doc_id\"] in new_docs:\n",
    "            item['ques_id'] = new_id\n",
    "            json.dump(item, new_docs_file)\n",
    "            new_docs_file.write('\\n')\n",
    "            new_id += 1\n",
    "        elif item[\"doc_id\"] in tune_docs:\n",
    "            item['ques_id'] = tune_id\n",
    "            json.dump(item, tune_docs_file)\n",
    "            tune_docs_file.write('\\n')\n",
    "            tune_id += 1\n",
    "\n",
    "    print(f\"Type of file: {type_file}\")\n",
    "    print(f\"Number in Old Docs: {old_id}\")\n",
    "    print(f\"Number in New Docs: {new_id}\")\n",
    "    print(f\"Number in Tune Docs: {tune_id}\")\n",
    "    print(f\"Total: {old_id+new_id+tune_id}\")\n",
    "    \n",
    "    \n",
    "    old_docs_file.close()\n",
    "    new_docs_file.close()\n",
    "    tune_docs_file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8290457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221194/221194 [00:02<00:00, 88114.83it/s]\n",
      "100%|██████████| 55299/55299 [00:00<00:00, 89953.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of folder: old_docs\n",
      "Number in Train: 221194\n",
      "Number in Val: 55299\n",
      "Total: 276493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22178/22178 [00:00<00:00, 89680.41it/s]\n",
      "100%|██████████| 5545/5545 [00:00<00:00, 89789.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of folder: new_docs\n",
      "Number in Train: 22178\n",
      "Number in Val: 5545\n",
      "Total: 27723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2525/2525 [00:00<00:00, 53899.02it/s]\n",
      "100%|██████████| 632/632 [00:00<00:00, 56557.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of folder: tune_docs\n",
      "Number in Train: 2525\n",
      "Number in Val: 632\n",
      "Total: 3157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# split train queries into train and val set using a 80-20 split\n",
    "new_folder = '/home/vk352/dsi/data/NQ320k/'\n",
    "folders = ['old_docs', 'new_docs', 'tune_docs']\n",
    "for folder in folders:\n",
    "    docs_file = open(os.path.join(new_folder, folder, f'naturalqueries.json'), 'r')\n",
    "    docs_file_train = open(os.path.join(new_folder, folder, f'trainqueries.json'), 'w')\n",
    "    docs_file_val = open(os.path.join(new_folder, folder, f'valqueries.json'), 'w')\n",
    "    \n",
    "    train_id = val_id = 0\n",
    "    \n",
    "    lines = docs_file.readlines()\n",
    "    perm_split = np.random.permutation(len(lines))\n",
    "    split = int(0.8*len(lines))\n",
    "    for index in tqdm(perm_split[:split]):\n",
    "        item = json.loads(lines[index])\n",
    "        item['ques_id'] = train_id\n",
    "        json.dump(item, docs_file_train)\n",
    "        docs_file_train.write('\\n')\n",
    "        train_id += 1\n",
    "        \n",
    "    for index in tqdm(perm_split[split:]):\n",
    "        item = json.loads(lines[index])\n",
    "        item['ques_id'] = val_id\n",
    "        json.dump(item, docs_file_val)\n",
    "        docs_file_val.write('\\n')\n",
    "        val_id += 1\n",
    "\n",
    "    print(f\"Type of folder: {folder}\")\n",
    "    print(f\"Number in Train: {train_id}\")\n",
    "    print(f\"Number in Val: {val_id}\")\n",
    "    print(f\"Total: {train_id+val_id}\")\n",
    "\n",
    "\n",
    "    docs_file.close()\n",
    "    docs_file_train.close()\n",
    "    docs_file_val.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b06765f",
   "metadata": {},
   "source": [
    "# write mappings for ANCE/DPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8fddeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write mapping\n",
    "docid2quesid = {}\n",
    "quesid2docid = {}\n",
    "\n",
    "file1 = open('/home/vk352/dsi/data/NQ320k/old_docs/passages_seen.json', 'r')\n",
    "lines = file1.readlines()\n",
    "for line in lines:\n",
    "    item = json.loads(line)\n",
    "    quesid2docid[item['gen_ques_id']] = item['doc_id']\n",
    "    if item['doc_id'] not in docid2quesid:\n",
    "        docid2quesid[item['doc_id']] = [item['gen_ques_id']]\n",
    "    else:\n",
    "        docid2quesid[item['doc_id']].append(item['gen_ques_id'])\n",
    "        \n",
    "\n",
    "with open('/home/vk352/dsi/data/NQ320k/old_docs/quesid2docid.pkl', 'wb') as f:\n",
    "    pickle.dump(quesid2docid, f)\n",
    "with open('/home/vk352/dsi/data/NQ320k/old_docs/docid2quesid.pkl', 'wb') as f:\n",
    "    pickle.dump(docid2quesid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828e404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc61dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8356bef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSI",
   "language": "python",
   "name": "dsi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
